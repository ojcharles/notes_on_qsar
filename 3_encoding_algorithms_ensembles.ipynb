{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Many encodings, many algorithms, and ensembles.\n",
    "\n",
    "\n",
    "In the last example 23 used HIV1 protease inhibition data to compare the QSAR modelling performance when using different descriptors, why did we not iterate through ML algorithms? Let's do that here!\n",
    "\n",
    "You will then learn that using the pooled output of many models can often outperform any given model, this has the term \" an ensemble model\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cheminformatics\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import deepchem as dc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning as previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>pKi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C#CCN1C(=O)N(CC#C)[C@H](Cc2ccccc2)[C@@H]2OC(C)...</td>\n",
       "      <td>1.342423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C#CCN1C(=O)N(CC#C)[C@H](Cc2ccccc2)[C@H](O)[C@@...</td>\n",
       "      <td>1.342423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C/C(=C\\C(=O)N(C)[C@@H](Cc1ccccc1)[C@H](O)CN(Cc...</td>\n",
       "      <td>1.720490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C/C(=N/O)c1cccc(CN2C(=O)N(Cc3cccc(/C(C)=N/O)c3...</td>\n",
       "      <td>-1.744727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C/C(=N/O)c1cccc(CN2[C@H](COc3ccccc3)[C@H](O)[C...</td>\n",
       "      <td>0.531479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>O[C@H]1C[C@H](Cc2ccccc2)[C@H](O)[C@@H](Cc2cccc...</td>\n",
       "      <td>6.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>S=C(NCCOc1nc2cc(Cl)ccc2n2cccc12)Nc1ccc(Br)cn1</td>\n",
       "      <td>1.845098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>[N-]=[N+]=N[C@@H](Cc1ccccc1)[C@@H]1[C@@H](O)[C...</td>\n",
       "      <td>2.518514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>[N-]=[N+]=Nc1ccc(S(=O)(=O)Nc2cccc(C(c3c(O)oc4c...</td>\n",
       "      <td>0.414973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>[O-][S+]1C[C@](CO)(Cc2ccccc2)C(O)[C@@](CO)(Cc2...</td>\n",
       "      <td>4.113943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2529 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles       pKi\n",
       "0     C#CCN1C(=O)N(CC#C)[C@H](Cc2ccccc2)[C@@H]2OC(C)...  1.342423\n",
       "1     C#CCN1C(=O)N(CC#C)[C@H](Cc2ccccc2)[C@H](O)[C@@...  1.342423\n",
       "2     C/C(=C\\C(=O)N(C)[C@@H](Cc1ccccc1)[C@H](O)CN(Cc...  1.720490\n",
       "3     C/C(=N/O)c1cccc(CN2C(=O)N(Cc3cccc(/C(C)=N/O)c3... -1.744727\n",
       "4     C/C(=N/O)c1cccc(CN2[C@H](COc3ccccc3)[C@H](O)[C...  0.531479\n",
       "...                                                 ...       ...\n",
       "2524  O[C@H]1C[C@H](Cc2ccccc2)[C@H](O)[C@@H](Cc2cccc...  6.301030\n",
       "2525      S=C(NCCOc1nc2cc(Cl)ccc2n2cccc12)Nc1ccc(Br)cn1  1.845098\n",
       "2526  [N-]=[N+]=N[C@@H](Cc1ccccc1)[C@@H]1[C@@H](O)[C...  2.518514\n",
       "2527  [N-]=[N+]=Nc1ccc(S(=O)(=O)Nc2cccc(C(c3c(O)oc4c...  0.414973\n",
       "2528  [O-][S+]1C[C@](CO)(Cc2ccccc2)C(O)[C@@](CO)(Cc2...  4.113943\n",
       "\n",
       "[2529 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('lib/data/2_hiv_protease.tsv')\n",
    "# some entries are missing label values\n",
    "df = df[df['Standard Value'] > 0]\n",
    "# there are duplicate values, let's get the mean Ki per smiles\n",
    "df = pd.DataFrame( df.groupby(['Smiles'])['Standard Value'].mean() )\n",
    "# now the Ki values are very diverse, let's take the log10 transform\n",
    "data = {'smiles': df['Standard Value'].index,\n",
    "        'pKi': np.log10(df['Standard Value'].values)}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions to help calculate vectors in parallel\n",
    "def smiles_2_rdkitDescr(smiles):\n",
    "    featurizer = dc.feat.RDKitDescriptors()\n",
    "    return(featurizer(smiles).flatten())\n",
    "\n",
    "def smiles_2_ecfp3(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol,3,nBits=1024,useFeatures=True)\n",
    "    arr = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return(arr)\n",
    "\n",
    "def smiles_2_ecfp4(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol,4,nBits=1024,useFeatures=True)\n",
    "    arr = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return(arr)\n",
    "\n",
    "def smiles_2_MACCS(smiles):\n",
    "    featurizer = dc.feat.MACCSKeysFingerprint()\n",
    "    return(featurizer(smiles).flatten())\n",
    "\n",
    "def smiles_2_mordredDescr(smiles):\n",
    "    featurizer = dc.feat.MordredDescriptors(ignore_3D=False)\n",
    "    return(featurizer(smiles).flatten())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we generate the vector of vectors for each type of descriptor\n",
    "df = df[0:200]\n",
    "y = df['pKi']\n",
    "\n",
    "# rdkit descriptors - 208 total\n",
    "with Pool(processes=os.cpu_count()) as p:\n",
    "    x_rdkit = np.stack(p.map(smiles_2_rdkitDescr, df.smiles.values))\n",
    "\n",
    "# Extended fingerprint\n",
    "with Pool(processes=os.cpu_count()) as p:\n",
    "    x_ecfp3 = np.stack(p.map(smiles_2_ecfp3, df.smiles.values))\n",
    "\n",
    "with Pool(processes=os.cpu_count()) as p:\n",
    "    x_ecfp4 = np.stack(p.map(smiles_2_ecfp4, df.smiles.values))\n",
    "\n",
    "# MACCS\n",
    "with Pool(processes=os.cpu_count()) as p:\n",
    "    x_MACCS = np.stack(p.map(smiles_2_MACCS, df.smiles.values))\n",
    "\n",
    "# Mordred - commented out to save time\n",
    "#with Pool(processes=os.cpu_count()) as p:\n",
    "#    x_mordred = np.stack(p.map(smiles_2_mordredDescr, df.Smiles.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature - algorithm - RMSE - r2\n",
      "x_rdkit LR 1.867885e+21 -2.627160e+20\n",
      "x_rdkit KNN 8.969781 -0.052794\n",
      "x_rdkit DT 10.086065 -0.328299\n",
      "x_rdkit RF 5.321819 0.251444\n",
      "x_rdkit xgb3 6.968863 0.108608\n",
      "x_rdkit xgb4 6.684751 0.239276\n",
      "x_rdkit xgb5 7.041868 0.089502\n",
      "x_ecfp3 LR 1.502057e+27 -3.219055e+26\n",
      "x_ecfp3 KNN 4.667032 0.319367\n",
      "x_ecfp3 DT 6.871945 -0.00026\n",
      "x_ecfp3 RF 4.898402 0.24375\n",
      "x_ecfp3 xgb3 6.886354 0.061699\n",
      "x_ecfp3 xgb4 6.222483 0.288439\n",
      "x_ecfp3 xgb5 5.489033 0.3548\n",
      "x_ecfp4 LR 2.437789e+27 -3.833814e+26\n",
      "x_ecfp4 KNN 4.193992 0.422053\n",
      "x_ecfp4 DT 6.99137 0.186972\n",
      "x_ecfp4 RF 5.548642 0.368625\n",
      "x_ecfp4 xgb3 6.200743 0.049698\n",
      "x_ecfp4 xgb4 5.878062 0.097405\n",
      "x_ecfp4 xgb5 5.527232 0.344559\n",
      "x_MACCS LR 8.378004e+25 -7.127467e+24\n",
      "x_MACCS KNN 5.651942 0.245493\n",
      "x_MACCS DT 10.510051 -0.485777\n",
      "x_MACCS RF 5.820756 0.247792\n",
      "x_MACCS xgb3 7.615086 -0.047532\n",
      "x_MACCS xgb4 5.854964 0.160063\n",
      "x_MACCS xgb5 6.679641 0.069235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "dict_algorithm = {\n",
    "    \"LR\": LinearRegression(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"DT\": DecisionTreeRegressor(max_depth=8),\n",
    "    \"RF\": RandomForestRegressor(),\n",
    "    \"xgb3\": xgb.XGBRegressor(n_estimators=200, max_depth=3),\n",
    "    \"xgb4\": xgb.XGBRFRegressor(n_estimators=200, max_depth=4),\n",
    "    \"xgb5\": xgb.XGBRFRegressor(n_estimators=200, max_depth=5),\n",
    "}\n",
    "\n",
    "dict_feats = {\n",
    "    'x_rdkit': x_rdkit,\n",
    "    'x_ecfp3': x_ecfp3,\n",
    "    'x_ecfp4': x_ecfp4,\n",
    "    'x_MACCS': x_MACCS,\n",
    "    #'x_mordred': x_mordred,    # commented out to save time\n",
    "}\n",
    "\n",
    "print('feature - algorithm - RMSE - r2')\n",
    "\n",
    "resample_num = 10\n",
    "\n",
    "# for each molvector type\n",
    "for f, f_df in dict_feats.items():\n",
    "\n",
    "    # for each algorithm\n",
    "    for m, model_instantiation in dict_algorithm.items():\n",
    "\n",
    "        #traack metrics\n",
    "        rmse_total = 0\n",
    "        r2_total = 0\n",
    "\n",
    "        for rep in range(resample_num):\n",
    "            \n",
    "            # each resmaple produces a novel dataset and model\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                f_df,\n",
    "                y,\n",
    "                test_size=0.3,\n",
    "                random_state=random.randint(1,999))\n",
    "            model_instantiation.fit(x_train, y_train)\n",
    "\n",
    "            # tot up metrics\n",
    "            test_preds = model_instantiation.predict(x_test)\n",
    "            col1 = f\n",
    "            col2 = m\n",
    "            rmse_total = rmse_total + mean_squared_error(y_test, test_preds)\n",
    "            r2_total = r2_total + r2_score(y_test, test_preds)\n",
    "        \n",
    "        # get mean summary stats\n",
    "        mean_rmse = rmse_total / resample_num\n",
    "        mean_r2 = r2_total / resample_num\n",
    "\n",
    "        newrow = pd.DataFrame( [[col1, col2, mean_rmse, mean_r2]] ,columns=['feat', 'model', 'RMSE', 'r2']) \n",
    "        print(newrow.to_string(index=False, header=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what did we learn?\n",
    "\n",
    "Here we took a problem, predicting the activity of HIV1 protease inhibitors. And using a set of classic ML QSAR models, tried to find the optimal balance of encoding and algorithm in order to find the best model.\n",
    "In this example the stochastic nature of the test:train split between different runs means each of us may have a different best model. I actually found KNN with ecfp4 the best, this may be as the data has a clumpy chemical space, with groups of active frameworks, and less active frameworks, but more on visualisation in another chapter.\n",
    "\n",
    "How could we overcome this? Well by increasing the resample_num to a much higher number means we can get a much better resolution on the probability distributions of the performance metrics. Try this at home.\n",
    "\n",
    "\n",
    "Now this is fairly comprehensive as far as classicML goes, but we can do a little better. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The endgame of classicML, the ensemble model\n",
    "\n",
    "Ensemlbe tree models, expand on decison trees. Rather thatn rely one \"well\" trained tree, they rely on many sub-optimal tree's then collate the output. Think of this as a medical problem, rather than taking one consultants opinion you instead take the average opinion of a diverse pool of junior doctors. Now why  stop your analogy there? If we asked for the opinions of many junior doctors across many hospitals, and totted dup their votes we could (should?) achieve better predictive power. This is what ensemble models (as in a model of models) try to do, Let's build one here!\n",
    "\n",
    "Now what we will do, it take the best performing models from above, re-run them, but collate their outputs.\n",
    "Let's use the xgb4 algorthm, and sum over all the different descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE - r2\n",
      "4.478775 0.49514\n",
      "3.93618 0.501792\n",
      "2.570388 0.607279\n",
      "4.560828 0.51086\n",
      "4.98882 0.376435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "dict_algorithm = {\n",
    "    #\"xgb3\": xgb.XGBRFRegressor(n_estimators=200, max_depth=3),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "}\n",
    "\n",
    "dict_feats = {\n",
    "    'x_rdkit': x_rdkit,\n",
    "    'x_ecfp3': x_ecfp3,\n",
    "    'x_ecfp4': x_ecfp4,\n",
    "    'x_MACCS': x_MACCS,\n",
    "    #'x_mordred': x_mordred,\n",
    "}\n",
    "\n",
    "print(' RMSE - r2')\n",
    "\n",
    "resample_num = 5\n",
    "\n",
    "\n",
    "\n",
    "for rep in range(resample_num):\n",
    "    i = 0\n",
    "\n",
    "    #traack metrics\n",
    "    rmse_total = 0\n",
    "    r2_total = 0\n",
    "\n",
    "    r_state = random.randint(1,999)\n",
    "         \n",
    "    # for each molvector type\n",
    "    for f, f_df in dict_feats.items():\n",
    "\n",
    "        # each resmaple produces a novel dataset and model\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            f_df,\n",
    "            y,\n",
    "            test_size=0.3,\n",
    "            random_state=r_state)\n",
    "\n",
    "        # for each algorithm\n",
    "        for m, model_instantiation in dict_algorithm.items():\n",
    "\n",
    "            # get the sum of prediction value per test point\n",
    "            model_instantiation.fit(x_train, y_train)\n",
    "            if i == 0:\n",
    "                test_preds = model_instantiation.predict(x_test)\n",
    "            else:\n",
    "                test_preds = test_preds + model_instantiation.predict(x_test)\n",
    "            i = i+1\n",
    "        \n",
    "    # now get the mean predicted value per test datapoint\n",
    "    test_preds = test_preds / len(dict_feats)\n",
    "\n",
    "    rmse_total = rmse_total + mean_squared_error(y_test, test_preds)\n",
    "    r2_total = r2_total + r2_score(y_test, test_preds)\n",
    "        \n",
    "    # get mean summary stats\n",
    "    mean_rmse = rmse_total\n",
    "    mean_r2 = r2_total \n",
    "\n",
    "    newrow = pd.DataFrame( [[mean_rmse, mean_r2]] ,columns=['RMSE', 'r2']) \n",
    "    print(newrow.to_string(index=False, header=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, you can see that ensemble models, where we take the pooled output from many different models, is (or normally should be) an improvement on any given model. \n",
    "\n",
    "\n",
    "Try tinkering with the above code block, can you find the best combination?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
